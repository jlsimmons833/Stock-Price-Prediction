{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "#import helper\n",
    "import time\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "\n",
    "import pprint\n",
    "\n",
    "\n",
    "def normalize_windows(win_data):\n",
    "    \"\"\" Normalize a window\n",
    "    Input: Window Data\n",
    "    Output: Normalized Window\n",
    "\n",
    "    Note: Run from load_data()\n",
    "\n",
    "    Note: Normalization data using n_i = (p_i / p_0) - 1,\n",
    "    denormalization using p_i = p_0(n_i + 1)\n",
    "    \"\"\"\n",
    "    norm_data = []\n",
    "    for w in win_data:\n",
    "        norm_win = [((float(p) / float(w[0])) - 1) for p in w]\n",
    "        norm_data.append(norm_win)\n",
    "    return norm_data\n",
    "\n",
    "\n",
    "def load_data(filename, seq_len, norm_win):\n",
    "    \"\"\"\n",
    "    Loads the data from a csv file into arrays\n",
    "\n",
    "    Input: Filename, sequence Length, normalization window(True, False)\n",
    "    Output: X_tr, Y_tr, X_te, Y_te\n",
    "\n",
    "    Note: Normalization data using n_i = (p_i / p_0) - 1,\n",
    "    denormalization using p_i = p_0(n_i + 1)\n",
    "\n",
    "    Note: Run from timeSeriesPredict.py\n",
    "    \"\"\"\n",
    "    fid = open(filename, 'r').read()\n",
    "    data = fid.split('\\n')\n",
    "    sequence_length = seq_len + 1\n",
    "    out = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        out.append(data[i: i + sequence_length])\n",
    "    if norm_win:\n",
    "        out = normalize_windows(out)\n",
    "    out = np.array(out)\n",
    "    split_ratio = 0.9\n",
    "    split = round(split_ratio * out.shape[0])\n",
    "    train = out[:int(split), :]\n",
    "    np.random.shuffle(train)\n",
    "    X_tr = train[:, :-1]\n",
    "    Y_tr = train[:, -1]\n",
    "    X_te = out[int(split):, :-1]\n",
    "    Y_te = out[int(split):, -1]\n",
    "    X_tr = np.reshape(X_tr, (X_tr.shape[0], X_tr.shape[1], 1))\n",
    "    X_te = np.reshape(X_te, (X_te.shape[0], X_te.shape[1], 1))\n",
    "    return [X_tr, Y_tr, X_te, Y_te]\n",
    "\n",
    "\n",
    "def predict_seq_mul(model, data, win_size, pred_len):\n",
    "    \"\"\"\n",
    "    Predicts multiple sequences\n",
    "    Input: keras model, testing data, window size, prediction length\n",
    "    Output: Predicted sequence\n",
    "\n",
    "    Note: Run from timeSeriesPredict.py\n",
    "    \"\"\"\n",
    "    pred_seq = []\n",
    "    for i in range(len(data)//pred_len):\n",
    "        current = data[i * pred_len]\n",
    "        predicted = []\n",
    "        for j in range(pred_len):\n",
    "            predicted.append(model.predict(current[None, :, :])[0, 0])\n",
    "            current = current[1:]\n",
    "            current = np.insert(current, [win_size - 1], predicted[-1], axis=0)\n",
    "        pred_seq.append(predicted)\n",
    "    return pred_seq\n",
    "\n",
    "\n",
    "def predict_pt_pt(model, data):\n",
    "    \"\"\"\n",
    "    Predicts only one timestep ahead\n",
    "    Input: keras model, testing data\n",
    "    Output: Predicted sequence\n",
    "\n",
    "    Note: Run from timeSeriesPredict.py\n",
    "    \"\"\"\n",
    "    predicted = model.predict(data)\n",
    "    predicted = np.reshape(predicted, (predicted.size, ))\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def plot_mul(Y_hat, Y, pred_len):\n",
    "    \"\"\"\n",
    "    PLots the predicted data versus true data\n",
    "\n",
    "    Input: Predicted data, True Data, Length of prediction\n",
    "    Output: return plot\n",
    "\n",
    "    Note: Run from timeSeriesPredict.py\n",
    "    \"\"\"\n",
    "    fig = plt.figure(facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(Y, label='Y')\n",
    "    # Print the predictions in its respective series-length\n",
    "    for i, j in enumerate(Y_hat):\n",
    "        shift = [None for p in range(i * pred_len)]\n",
    "        plt.plot(shift + j, label='Y_hat')\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For debugging purposes, the current directory is c:\\Users\\jlsim\\OneDrive\\Documents\\codingfolder\\Tradingtools\\Stock-Price-Prediction\\src\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "seq_len = 50\n",
    "norm_win = True\n",
    "print(\"For debugging purposes, the current directory is\",os.getcwd())\n",
    "#filename = './../sp500_prices.csv'\n",
    "filename = './../../sp500_prices.csv'\n",
    "X_tr, Y_tr, X_te, Y_te = load_data(filename, seq_len, norm_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built in:  0.016638517379760742\n"
     ]
    }
   ],
   "source": [
    "# Model Build\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_dim=1,\n",
    "               units=seq_len,\n",
    "               return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100,\n",
    "               return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))  # Linear dense layer to aggregate into 1 val\n",
    "model.add(Activation('linear'))\n",
    "timer_start = time.time()\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "print('Model built in: ', time.time()-timer_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "41/41 [==============================] - 9s 230ms/step - loss: 6.9308e-04 - val_loss: 5.3468e-04\n",
      "Epoch 2/200\n",
      "41/41 [==============================] - 9s 226ms/step - loss: 6.7498e-04 - val_loss: 6.9327e-04\n",
      "Epoch 3/200\n",
      "41/41 [==============================] - 9s 226ms/step - loss: 6.5442e-04 - val_loss: 7.3587e-04\n",
      "Epoch 4/200\n",
      "41/41 [==============================] - 9s 225ms/step - loss: 6.4174e-04 - val_loss: 5.5402e-04\n",
      "Epoch 5/200\n",
      "41/41 [==============================] - 9s 220ms/step - loss: 6.1903e-04 - val_loss: 4.8967e-04\n",
      "Epoch 6/200\n",
      "41/41 [==============================] - 6s 145ms/step - loss: 6.2747e-04 - val_loss: 4.8064e-04\n",
      "Epoch 7/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 6.1622e-04 - val_loss: 5.5671e-04\n",
      "Epoch 8/200\n",
      "41/41 [==============================] - 6s 142ms/step - loss: 6.0890e-04 - val_loss: 7.6474e-04\n",
      "Epoch 9/200\n",
      "41/41 [==============================] - 6s 144ms/step - loss: 5.9695e-04 - val_loss: 4.7204e-04\n",
      "Epoch 10/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 5.7931e-04 - val_loss: 4.2487e-04\n",
      "Epoch 11/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 5.6825e-04 - val_loss: 6.6736e-04\n",
      "Epoch 12/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 5.5221e-04 - val_loss: 6.1901e-04\n",
      "Epoch 13/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 5.4738e-04 - val_loss: 4.7799e-04\n",
      "Epoch 14/200\n",
      "41/41 [==============================] - 6s 146ms/step - loss: 5.3696e-04 - val_loss: 6.1932e-04\n",
      "Epoch 15/200\n",
      "41/41 [==============================] - 6s 144ms/step - loss: 5.3982e-04 - val_loss: 6.6396e-04\n",
      "Epoch 16/200\n",
      "41/41 [==============================] - 6s 143ms/step - loss: 5.1657e-04 - val_loss: 4.7656e-04\n",
      "Epoch 17/200\n",
      "41/41 [==============================] - 6s 146ms/step - loss: 5.5756e-04 - val_loss: 4.3540e-04\n",
      "Epoch 18/200\n",
      "41/41 [==============================] - 6s 144ms/step - loss: 5.0814e-04 - val_loss: 7.9821e-04\n",
      "Epoch 19/200\n",
      "41/41 [==============================] - 6s 144ms/step - loss: 5.0561e-04 - val_loss: 4.8296e-04\n",
      "Epoch 20/200\n",
      "41/41 [==============================] - 6s 145ms/step - loss: 4.9688e-04 - val_loss: 3.8910e-04\n",
      "Epoch 21/200\n",
      "41/41 [==============================] - 6s 146ms/step - loss: 5.1208e-04 - val_loss: 4.9962e-04\n",
      "Epoch 22/200\n",
      "41/41 [==============================] - 6s 155ms/step - loss: 4.9847e-04 - val_loss: 5.1932e-04\n",
      "Epoch 23/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 5.0024e-04 - val_loss: 5.4034e-04\n",
      "Epoch 24/200\n",
      "41/41 [==============================] - 6s 147ms/step - loss: 4.9255e-04 - val_loss: 3.7709e-04\n",
      "Epoch 25/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 4.8962e-04 - val_loss: 3.5059e-04\n",
      "Epoch 26/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 4.7408e-04 - val_loss: 4.0751e-04\n",
      "Epoch 27/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 4.7159e-04 - val_loss: 3.8270e-04\n",
      "Epoch 28/200\n",
      "41/41 [==============================] - 6s 157ms/step - loss: 4.8551e-04 - val_loss: 4.0441e-04\n",
      "Epoch 29/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 4.6028e-04 - val_loss: 3.6631e-04\n",
      "Epoch 30/200\n",
      "41/41 [==============================] - 6s 147ms/step - loss: 4.6705e-04 - val_loss: 4.1225e-04\n",
      "Epoch 31/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 4.7743e-04 - val_loss: 5.5794e-04\n",
      "Epoch 32/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 4.5439e-04 - val_loss: 5.7810e-04\n",
      "Epoch 33/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 4.6653e-04 - val_loss: 3.9359e-04\n",
      "Epoch 34/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 4.4434e-04 - val_loss: 3.4426e-04\n",
      "Epoch 35/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 4.5332e-04 - val_loss: 3.9270e-04\n",
      "Epoch 36/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 4.5050e-04 - val_loss: 3.7096e-04\n",
      "Epoch 37/200\n",
      "41/41 [==============================] - 6s 147ms/step - loss: 4.4787e-04 - val_loss: 3.5070e-04\n",
      "Epoch 38/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 4.4176e-04 - val_loss: 3.5192e-04\n",
      "Epoch 39/200\n",
      "41/41 [==============================] - 6s 147ms/step - loss: 4.4226e-04 - val_loss: 3.2061e-04\n",
      "Epoch 40/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 4.3398e-04 - val_loss: 3.7466e-04\n",
      "Epoch 41/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 4.2368e-04 - val_loss: 3.6775e-04\n",
      "Epoch 42/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 4.3083e-04 - val_loss: 3.3952e-04\n",
      "Epoch 43/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 4.2968e-04 - val_loss: 3.2320e-04\n",
      "Epoch 44/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 4.2193e-04 - val_loss: 5.4531e-04\n",
      "Epoch 45/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 4.2921e-04 - val_loss: 3.3529e-04\n",
      "Epoch 46/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 4.2543e-04 - val_loss: 4.3587e-04\n",
      "Epoch 47/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 4.2045e-04 - val_loss: 4.8076e-04\n",
      "Epoch 48/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 4.0992e-04 - val_loss: 3.4014e-04\n",
      "Epoch 49/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 4.0961e-04 - val_loss: 3.3175e-04\n",
      "Epoch 50/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 4.0216e-04 - val_loss: 3.3143e-04\n",
      "Epoch 51/200\n",
      "41/41 [==============================] - 7s 163ms/step - loss: 4.1315e-04 - val_loss: 3.1069e-04\n",
      "Epoch 52/200\n",
      "41/41 [==============================] - 6s 158ms/step - loss: 4.1158e-04 - val_loss: 3.4625e-04\n",
      "Epoch 53/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 4.1710e-04 - val_loss: 3.1701e-04\n",
      "Epoch 54/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.9596e-04 - val_loss: 2.9242e-04\n",
      "Epoch 55/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 4.0041e-04 - val_loss: 4.3732e-04\n",
      "Epoch 56/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 4.2073e-04 - val_loss: 4.5194e-04\n",
      "Epoch 57/200\n",
      "41/41 [==============================] - 6s 157ms/step - loss: 4.0228e-04 - val_loss: 3.4916e-04\n",
      "Epoch 58/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.9232e-04 - val_loss: 3.9069e-04\n",
      "Epoch 59/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.9699e-04 - val_loss: 3.1761e-04\n",
      "Epoch 60/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 3.9329e-04 - val_loss: 2.8299e-04\n",
      "Epoch 61/200\n",
      "41/41 [==============================] - 6s 155ms/step - loss: 3.8872e-04 - val_loss: 3.0037e-04\n",
      "Epoch 62/200\n",
      "41/41 [==============================] - 6s 155ms/step - loss: 3.8454e-04 - val_loss: 2.9630e-04\n",
      "Epoch 63/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 3.8615e-04 - val_loss: 2.8633e-04\n",
      "Epoch 64/200\n",
      "41/41 [==============================] - 6s 153ms/step - loss: 3.7859e-04 - val_loss: 3.0269e-04\n",
      "Epoch 65/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 3.8170e-04 - val_loss: 2.8582e-04\n",
      "Epoch 66/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.7591e-04 - val_loss: 3.0451e-04\n",
      "Epoch 67/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.8950e-04 - val_loss: 4.3139e-04\n",
      "Epoch 68/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.7664e-04 - val_loss: 3.4147e-04\n",
      "Epoch 69/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.8346e-04 - val_loss: 2.7350e-04\n",
      "Epoch 70/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.7303e-04 - val_loss: 2.7182e-04\n",
      "Epoch 71/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.7089e-04 - val_loss: 3.3776e-04\n",
      "Epoch 72/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.6324e-04 - val_loss: 4.2723e-04\n",
      "Epoch 73/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.6996e-04 - val_loss: 3.0300e-04\n",
      "Epoch 74/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.7842e-04 - val_loss: 4.4367e-04\n",
      "Epoch 75/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 3.6520e-04 - val_loss: 3.5308e-04\n",
      "Epoch 76/200\n",
      "41/41 [==============================] - 6s 147ms/step - loss: 3.6146e-04 - val_loss: 2.6912e-04\n",
      "Epoch 77/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.6017e-04 - val_loss: 3.0722e-04\n",
      "Epoch 78/200\n",
      "41/41 [==============================] - 6s 153ms/step - loss: 3.5795e-04 - val_loss: 3.3388e-04\n",
      "Epoch 79/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.5497e-04 - val_loss: 4.5271e-04\n",
      "Epoch 80/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 3.6160e-04 - val_loss: 2.8443e-04\n",
      "Epoch 81/200\n",
      "41/41 [==============================] - 6s 157ms/step - loss: 3.5112e-04 - val_loss: 2.7546e-04\n",
      "Epoch 82/200\n",
      "41/41 [==============================] - 6s 153ms/step - loss: 3.5746e-04 - val_loss: 3.3649e-04\n",
      "Epoch 83/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 3.5663e-04 - val_loss: 3.0437e-04\n",
      "Epoch 84/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.5213e-04 - val_loss: 2.7139e-04\n",
      "Epoch 85/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.5837e-04 - val_loss: 3.4186e-04\n",
      "Epoch 86/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.6180e-04 - val_loss: 2.4603e-04\n",
      "Epoch 87/200\n",
      "41/41 [==============================] - 6s 153ms/step - loss: 3.4627e-04 - val_loss: 2.6568e-04\n",
      "Epoch 88/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.4403e-04 - val_loss: 2.6685e-04\n",
      "Epoch 89/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.5422e-04 - val_loss: 2.9685e-04\n",
      "Epoch 90/200\n",
      "41/41 [==============================] - 6s 157ms/step - loss: 3.5658e-04 - val_loss: 3.0655e-04\n",
      "Epoch 91/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.4787e-04 - val_loss: 3.0358e-04\n",
      "Epoch 92/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.4783e-04 - val_loss: 2.7373e-04\n",
      "Epoch 93/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.4784e-04 - val_loss: 3.7903e-04\n",
      "Epoch 94/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.4939e-04 - val_loss: 2.6649e-04\n",
      "Epoch 95/200\n",
      "41/41 [==============================] - 6s 153ms/step - loss: 3.3940e-04 - val_loss: 3.2182e-04\n",
      "Epoch 96/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.3321e-04 - val_loss: 3.7905e-04\n",
      "Epoch 97/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 3.4611e-04 - val_loss: 3.1965e-04\n",
      "Epoch 98/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.5248e-04 - val_loss: 2.5592e-04\n",
      "Epoch 99/200\n",
      "41/41 [==============================] - 6s 153ms/step - loss: 3.4370e-04 - val_loss: 2.7652e-04\n",
      "Epoch 100/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.4793e-04 - val_loss: 2.4495e-04\n",
      "Epoch 101/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.2929e-04 - val_loss: 3.0481e-04\n",
      "Epoch 102/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.3275e-04 - val_loss: 3.9275e-04\n",
      "Epoch 103/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 3.4306e-04 - val_loss: 2.6663e-04\n",
      "Epoch 104/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 3.3306e-04 - val_loss: 2.5599e-04\n",
      "Epoch 105/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.3820e-04 - val_loss: 2.3866e-04\n",
      "Epoch 106/200\n",
      "41/41 [==============================] - 6s 155ms/step - loss: 3.4345e-04 - val_loss: 3.2207e-04\n",
      "Epoch 107/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 3.2809e-04 - val_loss: 2.7042e-04\n",
      "Epoch 108/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.2953e-04 - val_loss: 2.7056e-04\n",
      "Epoch 109/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.3162e-04 - val_loss: 2.3513e-04\n",
      "Epoch 110/200\n",
      "41/41 [==============================] - 7s 164ms/step - loss: 3.2954e-04 - val_loss: 2.7042e-04\n",
      "Epoch 111/200\n",
      "41/41 [==============================] - 6s 157ms/step - loss: 3.2698e-04 - val_loss: 2.3847e-04\n",
      "Epoch 112/200\n",
      "41/41 [==============================] - 6s 158ms/step - loss: 3.2885e-04 - val_loss: 2.5223e-04\n",
      "Epoch 113/200\n",
      "41/41 [==============================] - 6s 154ms/step - loss: 3.3898e-04 - val_loss: 2.6782e-04\n",
      "Epoch 114/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.2590e-04 - val_loss: 3.2091e-04\n",
      "Epoch 115/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 3.3868e-04 - val_loss: 3.0286e-04\n",
      "Epoch 116/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 3.2142e-04 - val_loss: 3.9849e-04\n",
      "Epoch 117/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.3118e-04 - val_loss: 2.8926e-04\n",
      "Epoch 118/200\n",
      "41/41 [==============================] - 6s 156ms/step - loss: 3.1646e-04 - val_loss: 2.8667e-04\n",
      "Epoch 119/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.2287e-04 - val_loss: 2.6206e-04\n",
      "Epoch 120/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.2156e-04 - val_loss: 2.7223e-04\n",
      "Epoch 121/200\n",
      "41/41 [==============================] - 6s 154ms/step - loss: 3.2129e-04 - val_loss: 2.6273e-04\n",
      "Epoch 122/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 3.1529e-04 - val_loss: 2.2877e-04\n",
      "Epoch 123/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 3.2445e-04 - val_loss: 2.3714e-04\n",
      "Epoch 124/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.0969e-04 - val_loss: 4.7700e-04\n",
      "Epoch 125/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.2197e-04 - val_loss: 3.4954e-04\n",
      "Epoch 126/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 3.1355e-04 - val_loss: 2.9794e-04\n",
      "Epoch 127/200\n",
      "41/41 [==============================] - 6s 146ms/step - loss: 3.0963e-04 - val_loss: 2.3011e-04\n",
      "Epoch 128/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.1384e-04 - val_loss: 2.2619e-04\n",
      "Epoch 129/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.1195e-04 - val_loss: 2.3518e-04\n",
      "Epoch 130/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.0433e-04 - val_loss: 2.5414e-04\n",
      "Epoch 131/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 3.1732e-04 - val_loss: 2.6576e-04\n",
      "Epoch 132/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 3.0809e-04 - val_loss: 2.5016e-04\n",
      "Epoch 133/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.0750e-04 - val_loss: 2.3835e-04\n",
      "Epoch 134/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 3.1286e-04 - val_loss: 3.0151e-04\n",
      "Epoch 135/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.1521e-04 - val_loss: 2.3683e-04\n",
      "Epoch 136/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 3.0153e-04 - val_loss: 3.7907e-04\n",
      "Epoch 137/200\n",
      "41/41 [==============================] - 7s 162ms/step - loss: 3.1958e-04 - val_loss: 2.0617e-04\n",
      "Epoch 138/200\n",
      "41/41 [==============================] - 6s 158ms/step - loss: 3.0445e-04 - val_loss: 5.1973e-04\n",
      "Epoch 139/200\n",
      "41/41 [==============================] - 6s 159ms/step - loss: 3.0610e-04 - val_loss: 2.5030e-04\n",
      "Epoch 140/200\n",
      "41/41 [==============================] - 6s 154ms/step - loss: 3.0158e-04 - val_loss: 2.3041e-04\n",
      "Epoch 141/200\n",
      "41/41 [==============================] - 6s 153ms/step - loss: 3.0557e-04 - val_loss: 3.0881e-04\n",
      "Epoch 142/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.9934e-04 - val_loss: 2.3607e-04\n",
      "Epoch 143/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 2.9920e-04 - val_loss: 2.0925e-04\n",
      "Epoch 144/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.1288e-04 - val_loss: 2.8245e-04\n",
      "Epoch 145/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 2.9947e-04 - val_loss: 2.1137e-04\n",
      "Epoch 146/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 2.9864e-04 - val_loss: 3.0349e-04\n",
      "Epoch 147/200\n",
      "41/41 [==============================] - 6s 154ms/step - loss: 3.0866e-04 - val_loss: 1.9948e-04\n",
      "Epoch 148/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 2.9772e-04 - val_loss: 2.4514e-04\n",
      "Epoch 149/200\n",
      "41/41 [==============================] - 6s 154ms/step - loss: 3.0148e-04 - val_loss: 2.5798e-04\n",
      "Epoch 150/200\n",
      "41/41 [==============================] - 6s 153ms/step - loss: 2.9439e-04 - val_loss: 2.3615e-04\n",
      "Epoch 151/200\n",
      "41/41 [==============================] - 6s 153ms/step - loss: 2.9650e-04 - val_loss: 2.2259e-04\n",
      "Epoch 152/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 2.9358e-04 - val_loss: 2.3905e-04\n",
      "Epoch 153/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 3.0591e-04 - val_loss: 2.4740e-04\n",
      "Epoch 154/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 2.9777e-04 - val_loss: 2.2853e-04\n",
      "Epoch 155/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 2.9738e-04 - val_loss: 2.9960e-04\n",
      "Epoch 156/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.9667e-04 - val_loss: 2.3631e-04\n",
      "Epoch 157/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.9497e-04 - val_loss: 2.1787e-04\n",
      "Epoch 158/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 2.8489e-04 - val_loss: 2.4926e-04\n",
      "Epoch 159/200\n",
      "41/41 [==============================] - 6s 154ms/step - loss: 2.9141e-04 - val_loss: 2.3226e-04\n",
      "Epoch 160/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.9816e-04 - val_loss: 2.0861e-04\n",
      "Epoch 161/200\n",
      "41/41 [==============================] - 6s 157ms/step - loss: 2.8996e-04 - val_loss: 3.0836e-04\n",
      "Epoch 162/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 2.8994e-04 - val_loss: 2.3396e-04\n",
      "Epoch 163/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 2.8935e-04 - val_loss: 2.7029e-04\n",
      "Epoch 164/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 2.8563e-04 - val_loss: 2.2724e-04\n",
      "Epoch 165/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.9143e-04 - val_loss: 2.2400e-04\n",
      "Epoch 166/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.8332e-04 - val_loss: 2.3132e-04\n",
      "Epoch 167/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.9176e-04 - val_loss: 2.4859e-04\n",
      "Epoch 168/200\n",
      "41/41 [==============================] - 6s 153ms/step - loss: 2.9976e-04 - val_loss: 2.2129e-04\n",
      "Epoch 169/200\n",
      "41/41 [==============================] - 6s 156ms/step - loss: 2.8841e-04 - val_loss: 2.2809e-04\n",
      "Epoch 170/200\n",
      "41/41 [==============================] - 6s 153ms/step - loss: 2.8234e-04 - val_loss: 2.5082e-04\n",
      "Epoch 171/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.8353e-04 - val_loss: 2.6362e-04\n",
      "Epoch 172/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.9192e-04 - val_loss: 2.0893e-04\n",
      "Epoch 173/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 2.8236e-04 - val_loss: 2.1449e-04\n",
      "Epoch 174/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.8682e-04 - val_loss: 2.1434e-04\n",
      "Epoch 175/200\n",
      "41/41 [==============================] - 6s 154ms/step - loss: 2.9068e-04 - val_loss: 2.6763e-04\n",
      "Epoch 176/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 2.8291e-04 - val_loss: 2.3174e-04\n",
      "Epoch 177/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 2.7752e-04 - val_loss: 1.8742e-04\n",
      "Epoch 178/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.7850e-04 - val_loss: 2.1747e-04\n",
      "Epoch 179/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 2.8623e-04 - val_loss: 2.0591e-04\n",
      "Epoch 180/200\n",
      "41/41 [==============================] - 6s 154ms/step - loss: 2.7047e-04 - val_loss: 2.6509e-04\n",
      "Epoch 181/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.8738e-04 - val_loss: 2.1159e-04\n",
      "Epoch 182/200\n",
      "41/41 [==============================] - 6s 155ms/step - loss: 2.7132e-04 - val_loss: 2.6992e-04\n",
      "Epoch 183/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 2.7616e-04 - val_loss: 2.6507e-04\n",
      "Epoch 184/200\n",
      "41/41 [==============================] - 6s 152ms/step - loss: 2.8011e-04 - val_loss: 2.0410e-04\n",
      "Epoch 185/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.7826e-04 - val_loss: 2.9691e-04\n",
      "Epoch 186/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.7817e-04 - val_loss: 2.1769e-04\n",
      "Epoch 187/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 2.8805e-04 - val_loss: 2.4243e-04\n",
      "Epoch 188/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.7687e-04 - val_loss: 2.2807e-04\n",
      "Epoch 189/200\n",
      "41/41 [==============================] - 6s 155ms/step - loss: 2.8070e-04 - val_loss: 2.0963e-04\n",
      "Epoch 190/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 2.8752e-04 - val_loss: 2.6257e-04\n",
      "Epoch 191/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 2.7325e-04 - val_loss: 2.5130e-04\n",
      "Epoch 192/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.7398e-04 - val_loss: 2.6610e-04\n",
      "Epoch 193/200\n",
      "41/41 [==============================] - 6s 147ms/step - loss: 2.7796e-04 - val_loss: 2.0295e-04\n",
      "Epoch 194/200\n",
      "41/41 [==============================] - 6s 148ms/step - loss: 2.7734e-04 - val_loss: 2.1746e-04\n",
      "Epoch 195/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.6558e-04 - val_loss: 2.3873e-04\n",
      "Epoch 196/200\n",
      "41/41 [==============================] - 6s 147ms/step - loss: 2.7778e-04 - val_loss: 2.2192e-04\n",
      "Epoch 197/200\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 2.8320e-04 - val_loss: 2.1754e-04\n",
      "Epoch 198/200\n",
      "41/41 [==============================] - 6s 158ms/step - loss: 2.7450e-04 - val_loss: 2.1996e-04\n",
      "Epoch 199/200\n",
      "41/41 [==============================] - 6s 150ms/step - loss: 2.7431e-04 - val_loss: 2.4191e-04\n",
      "Epoch 200/200\n",
      "41/41 [==============================] - 6s 151ms/step - loss: 2.7326e-04 - val_loss: 1.8648e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x145885a1510>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "model.fit(X_tr,\n",
    "          Y_tr,\n",
    "          batch_size=512,\n",
    "          epochs=200,\n",
    "          validation_split=0.05\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "win_size = seq_len\n",
    "pred_len = seq_len\n",
    "plot = True\n",
    "\n",
    "\n",
    "if plot:\n",
    "    pred = predict_seq_mul(model, X_te, win_size, pred_len)\n",
    "    plot_mul(pred, Y_te, pred_len)\n",
    "else:\n",
    "    pred = predict_pt_pt(model, X_te)\n",
    "    mse_model = mean_squared_error(Y_te, pred)\n",
    "    print(\"MSE of DL model \", mse_model)\n",
    "    # Stupid Model\n",
    "    y_bar = np.mean(X_te, axis=1)\n",
    "    y_bar = np.reshape(y_bar, (y_bar.shape[0]))\n",
    "    mse_base = mean_squared_error(Y_te, y_bar)\n",
    "    print(\"MSE of y_bar Model\", mse_base)\n",
    "    # t-1 Model\n",
    "    y_t_1 = X_te[:, -1]\n",
    "    y_t_1 = np.reshape(y_t_1, (y_t_1.shape[0]))\n",
    "    mse_t_1 = mean_squared_error(Y_te, y_t_1)\n",
    "    print(\"MSE of t-1 Model\", mse_t_1)\n",
    "    # Comparisons\n",
    "    improv = (mse_model - mse_base)/mse_base\n",
    "    improv_t_1 = (mse_model - mse_t_1)/mse_t_1\n",
    "    print(\"%ge improvement over naive model\", improv)\n",
    "    print(\"%ge improvement over t-1 model\", improv_t_1)\n",
    "    corr_model = np.corrcoef(Y_te, pred)\n",
    "    corr_base = np.corrcoef(Y_te, y_bar)\n",
    "    corr_t_1 = np.corrcoef(Y_te, y_t_1)\n",
    "    print(\"Correlation of y_bar \\n \", corr_base, \"\\n t-1 model \\n\", corr_t_1,\n",
    "          \"\\n DL model\\n\", corr_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
